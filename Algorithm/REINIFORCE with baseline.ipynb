{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the REINFORCE Algorithm in Sequence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation\n",
    "- $\\tau$: Action sequence $(a_{1}, a_{2}, \\dots)$\n",
    "- $\\pi_{\\theta}$: Policy parameterized by $\\theta$\n",
    "- $\\gamma$: Discount factor\n",
    "- $r(\\tau)$: Discounted total reward given action sequence\n",
    "\n",
    "Loss function:\n",
    "\n",
    "$L(\\theta) = -\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REIINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation of policy gradient:\n",
    "$$\\nabla_{\\theta}\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)] =\\mathop{\\Sigma}_{\\tau}r(\\tau)\\nabla_{\\theta}\\pi_{\\theta}(\\tau)\\\\\n",
    "=\\mathop{\\Sigma}_{\\tau}r(\\tau)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(\\tau)}{\\pi_{\\theta}(\\tau)}\\pi_{\\theta}(\\tau)\\\\\n",
    "=\\mathop{\\Sigma}_{\\tau}\\pi_{\\theta}(\\tau)\\cdot r(\\tau)\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)\\\\\n",
    "=\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}r(\\tau)\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)\\\\\n",
    "\\approx \\frac{1}{N}\\mathop{\\Sigma}_{i=1}^{N}r(\\tau_{i})\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the policy gradient by adding baseline reward $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)-b]\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expectation of policy gradient does **NOT** change given $b$ is independent of $\\tau$\n",
    "\n",
    "Proof:\n",
    "$$\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)-b]\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau) = \\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}r(\\tau)\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)-\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}b\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)$$\n",
    "in which\n",
    "$$\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}b\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)=\\mathop{\\Sigma}_{\\tau}b\\frac{\\nabla_{\\theta}\\pi_{\\theta}(\\tau)}{\\pi_{\\theta}(\\tau)}\\cdot \\pi_{\\theta}(\\tau)=b\\mathop{\\Sigma}_{\\tau}\\nabla_{\\theta}\\pi_{\\theta}(\\tau)=b\\nabla_{\\theta}\\mathop{\\Sigma}_{\\tau}\\pi_{\\theta}(\\tau)=b\\nabla_{\\theta}1=0\n",
    "$$\n",
    "- Variance of gradient estimate may got reduced\n",
    "\n",
    "Denote the $i_{th}$ entry of $\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)$ as $g_{\\theta_{i}}(\\tau)$.\n",
    "\n",
    "$\\frac{\\mathrm{d}\\mathbb{D}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[(r(\\tau)-b)\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau)]}{\\mathrm{d}b}=\\frac{\\mathrm{d}\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[(r(\\tau)-b)^{2}g_{\\theta_{i}}^{2}(\\tau)]}{\\mathrm{d}b}= \\frac{\\mathrm{d}\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r^{2}(\\tau)g_{\\theta_{i}}^{2}(\\tau)]+b^{2}\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[g_{\\theta_{i}}^{2}(\\tau)]-2b\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)g_{\\theta_{i}}^{2}(\\tau)]}{\\mathrm{d}b}=0+2b\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[g_{\\theta_{i}}^{2}(\\tau)]-2\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)g_{\\theta_{i}}^{2}(\\tau)]=0$, \n",
    "\n",
    "$b=\\frac{\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)g_{\\theta_{i}}^{2}(\\tau)]}{\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[g_{\\theta_{i}}^{2}(\\tau)]}$\n",
    "\n",
    "With a total of $N$ trails, the expectation can be approximated with\n",
    "$$\\mathop{\\mathbb{E}}_{\\tau\\sim \\pi_{\\theta}(\\tau)}[r(\\tau)-b]\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau) \\approx \\frac{1}{N}\\mathop{\\Sigma}_{i=1}^{N}[r(\\tau_{i})-b]\\nabla_{\\theta}\\ln \\pi_{\\theta}(\\tau_{i})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[Self-critical Sequence Training for Image Captioning](https://arxiv.org/abs/1612.00563)\n",
    "\n",
    "[A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit4aad875cf071498186715d87f6c3423a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
